{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp core\n",
    "# This will create a package named bifrost_bridge/core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev\n",
    "from nbdev.showdoc import *  # ignore this Pylance warning in favor of following nbdev docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For help with the Markdown language, see [this guide](https://www.markdownguide.org/basic-syntax/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global static vars\n",
    "These are used to modify the template for individual use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# Need the bifrost_bridge for a few functions, this can be considered a static var\n",
    "\n",
    "import importlib\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "PACKAGE_NAME: str = (\n",
    "    \"bifrost_bridge\"  # Make sure to adjust this to your package name\n",
    ")\n",
    "DEV_MODE: bool = (\n",
    "    False  # set below to override, as this is in an export block it'll be exported while the dev mode section is not\n",
    ")\n",
    "\n",
    "PACKAGE_DIR = None\n",
    "try:\n",
    "    spec = importlib.util.find_spec(PACKAGE_NAME)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    PACKAGE_DIR = os.path.dirname(module.__file__)\n",
    "except ImportError:\n",
    "    DEV_MODE = True\n",
    "except AttributeError:\n",
    "    DEV_MODE = True\n",
    "PROJECT_DIR = os.getcwd()  # override value in dev mode\n",
    "if PROJECT_DIR.endswith('nbs'):\n",
    "    DEV_MODE=True\n",
    "    PROJECT_DIR=os.path.split(PROJECT_DIR)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev mode\n",
    "If you're developing this versus running this, you'll have access to slightly different things. Notable the nbdev functions are only for development and not for runtime. This matters for items such as the config. So we need to detect if you are in dev mode or not and the code has to adjust accordingly. Notice that this section is not exported so will only work in the notebook and not in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section uses nbdev functions so should not be exported as it's for dev purposes\n",
    "import os\n",
    "\n",
    "if DEV_MODE:\n",
    "    PACKAGE_DIR = nbdev.config.get_config(cfg_name=\"settings.ini\", path=os.getcwd())[\n",
    "        \"lib_path\"\n",
    "    ]  # the library is the package of course\n",
    "    PROJECT_DIR = nbdev.config.get_config(\n",
    "        cfg_name=\"settings.ini\", path=os.getcwd()\n",
    "    ).config_path  # the default location of nbdev config file (settings.ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    " A module which contains common functions to be used by other modules. Those that exist in the template are meant to be common functions we can use against multiple packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#|hide\n",
    "\n",
    "Notebook blocks starting with #|hide are not shown in the documentation and not exported to the python package. Blocks with #|export are exported to the python package. Blocks with neither are shown to the documentation but not exported to the python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Currently all libraries included are listed at the top and calls to them are also made in the block of code that uses them. This is for readability and the performance hit of the import is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# standard libs\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Common to template\n",
    "# add into settings.ini, requirements, package name is python-dotenv, for conda build ensure `conda config --add channels conda-forge`\n",
    "import dotenv  # for loading config from .env files, https://pypi.org/project/python-dotenv/\n",
    "import envyaml  # Allows to loads env vars into a yaml file, https://github.com/thesimj/envyaml\n",
    "import fastcore  # To add functionality related to nbdev development, https://github.com/fastai/fastcore/\n",
    "import pandas  # For sample sheet manipulation\n",
    "from fastcore import (\n",
    "    test,\n",
    ")\n",
    "from fastcore.script import (\n",
    "    call_parse,\n",
    ")  # for @call_parse, https://fastcore.fast.ai/script\n",
    "\n",
    "# Project specific libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Our config file holds all program and user specific variables. This is a good practice to follow as it allows us to easily change variables without having to change code. It also allows us to easily change variables based on the environment we are running in. For example, we may want to run a program in a test environment with a different database than we would in production. This is also a good practice to follow as it allows us to easily change variables without having to change code. It also allows us to easily change variables based on the environment we are running in. For example, we may want to run a program in a test environment with a different database than we would in production.\n",
    "\n",
    "Configuration is templated to rely on environment (ENV) variables. A default ENV config is provided in `./config/config.default.env` and more advanced data structures are supported in `./config/config.default.yaml`. The `.yaml` file is meant to represent what your program actually works with and the `.env` file options the user can change at run time.\n",
    "\n",
    "Make sure you know the priority of variables and check on them when debugging your code. Also ensure that your yaml file is referenced appropriately in the `.env` file. \n",
    "\n",
    "When in use there's an expectation you'll have multiple config files for different use cases e.g. development, production environment for different paths, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set env variables\n",
    "A helper function for getting your config values, this will set the environment variables with the provided `.env` values. If you're missing values it'll ensure they're loaded in with the defaults file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import importlib\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "def set_env_variables(config_path: str, overide_env_vars: bool = True) -> bool:\n",
    "    # Load dot env sets environmental values from a file, if the value already exists it will not be overwritten unless override is set to True.\n",
    "    # If we have multiple .env files then we need to apply the one which we want to take precedence last with overide.\n",
    "\n",
    "    # Order of precedence: .env file > environment variables > default values\n",
    "    # When developing, making a change to the config will not be reflected until the environment is restarted\n",
    "\n",
    "    # Set the env vars first, this is needed for the card.yaml to replace ENV variables\n",
    "    # NOTE: You need to adjust PROJECT_NAME to your package name for this to work, the exception is only for dev purposes\n",
    "    # This here checks if your package is installed, such as through pypi or through pip install -e  [.dev] for development. If it is then it'll go there and use the config files there as your default values.\n",
    "    try:\n",
    "        dotenv.load_dotenv(\n",
    "            f\"{PACKAGE_DIR}/config/config.default.env\", override=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {PACKAGE_DIR}/config/config.default.env does not exist\")\n",
    "        return False\n",
    "\n",
    "    # 2. set values from file:\n",
    "    if os.path.isfile(config_path):\n",
    "        dotenv.load_dotenv(config_path, override=overide_env_vars)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get config\n",
    "\n",
    "When you run this function, assuming things are set up properly, you end up with a dict that matches your `.yaml` file. This file will have all the inputs for the package and settings of your program.\n",
    "\n",
    "To do this it will use a `.env` config file, which has an associated yaml file defined with `CORE_YAML_CONFIG_FILE` in the `.env` file. And then use the `.env` file to load values into the associated `.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import importlib\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "def get_config(config_path: str = None, overide_env_vars: bool = True) -> dict:\n",
    "    if config_path is None:\n",
    "        config_path = \"\"\n",
    "    # First sets environment with variables from config_path, then uses those variables to fill in appropriate values in the config.yaml file, the yaml file is then returned as a dict\n",
    "    # If you want user env variables to take precedence over the config.yaml file then set overide_env_vars to False\n",
    "    set_env_variables(config_path, overide_env_vars)\n",
    "   \n",
    "    config: dict = envyaml.EnvYAML(\n",
    "        os.environ.get(\n",
    "            \"CORE_YAML_CONFIG_FILE\", f\"{PACKAGE_DIR}/config/config.default.yaml\"\n",
    "        ),\n",
    "        strict=False,\n",
    "    ).export()\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "All the user input variables and machine adjustable variables should be in your config, which is a dict. Reference config.default.yaml for how to access your variables. Also note that with python dicts you can use `dict_variable.get(\"variable\", default_value)` to ensure that you don't get a key error if the variable is not set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# create a os.PathLike object\n",
    "config = get_config(os.environ.get(\"CORE_CONFIG_FILE\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show project env vars\n",
    "A helper function intended to only be used with debugging. It shows all your project specific environmental variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def show_project_env_vars(config: dict) -> None:\n",
    "    # Prints out all the project environment variables\n",
    "    # This is useful for debugging and seeing what is being set\n",
    "    for k, v in config.items():\n",
    "        # If ENV var starts with PROJECTNAME_ then print\n",
    "        if k.startswith(config[\"CORE_PROJECT_VARIABLE_PREFIX\"]):\n",
    "            print(f\"{k}={v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIFROST_BRIDGE_INPUT_DIR=./input\n",
      "BIFROST_BRIDGE_OUTPUT_DIR=./output\n",
      "BIFROST_BRIDGE_OUTPUT_FILE=./output/output.txt\n",
      "BIFROST_BRIDGE_USER_INPUT_NAME=Kim\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "show_project_env_vars(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_samplesheet\n",
    "This function is to unify the way we work with sample_sheet's which is for us a file with a table of values, typically samples for batch processing. We want to approach doing it this way so all programs have batch processing in mind and working with the same data structure.\n",
    "\n",
    "To make use of it we have a small sample_sheet yaml object which looks like\n",
    "    \n",
    "```yaml\n",
    "sample_sheet:\n",
    "    path: path/to/sample_sheet.tsv\n",
    "    delimiter: '\\t' # Optional, will assume , for csv and \\t otherwises\n",
    "    header: 0 # Optional, 0 indicates first row is header, None indicates no header\n",
    "    columns: ['column1', 'column2', 'column3'] # Optional, if not provided all columns will be used\n",
    "```\n",
    "\n",
    "Make sure to add that to your relevant section in your config (can be multiple times if you're working with different sheets or different columns), then call the function on this object and it'll either mention somethings wrong or return a pandas dataframe with the columns of interest.\n",
    "\n",
    "This is an example of a common sample_sheet we work with. We will ingest the hash at the beginning so it doesn't affect column naming. Extra empty rows at the end are also stripped.\n",
    "```tsv\n",
    "#sample_id\tfile_path\tmetadata1\tmetadata2\n",
    "Sample1\t/path/to/sample1.fasta\tvalue1\toption1\n",
    "Sample2\t/path/to/sample2.fasta\tvalue2\toption2\n",
    "Sample3\t/path/to/sample3.fasta\tvalue3\toption1\n",
    "Sample4\t/path/to/sample4.fasta\tvalue1\toption2\n",
    "Sample5\t/path/to/sample5.fasta\tvalue2\toption1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_samplesheet(sample_sheet_config: dict) -> pd.DataFrame:\n",
    "    # Load the sample sheet into a pandas dataframe\n",
    "    # If columns is not None then it will only load those columns\n",
    "    # If the sample sheet is a csv then it will load it as a csv, otherwise it will assume it's a tsv\n",
    "\n",
    "    # Expected sample_sheet_config:\n",
    "    # sample_sheet:\n",
    "    #   path: path/to/sample_sheet.tsv\n",
    "    #   delimiter: '\\t' # Optional, will assume , for csv and \\t otherwises\n",
    "    #   header: 0 # Optional, 0 indicates first row is header, None indicates no header\n",
    "    #   columns: ['column1', 'column2', 'column3'] # Optional, if not provided all columns will be used\n",
    "\n",
    "    # Example sample sheet:\n",
    "    # #sample_id\tfile_path\tmetadata1\tmetadata2\n",
    "    # Sample1\t/path/to/sample1.fasta\tvalue1\toption1\n",
    "    # Sample2\t/path/to/sample2.fasta\tvalue2\toption2\n",
    "    # Sample3\t/path/to/sample3.fasta\tvalue3\toption1\n",
    "    # Sample4\t/path/to/sample4.fasta\tvalue1\toption2\n",
    "    # Sample5\t/path/to/sample5.fasta\tvalue2\toption1\n",
    "\n",
    "    # This function should also handle ensuring the sample sheet is in the correct format, such as ensuring the columns are correct and that the sample names are unique.\n",
    "    if not os.path.isfile(sample_sheet_config[\"path\"]):\n",
    "        raise FileNotFoundError(f\"File {sample_sheet_config['path']} does not exist\")\n",
    "    if \"delimiter\" in sample_sheet_config:\n",
    "        delimiter = sample_sheet_config[\"delimiter\"]\n",
    "    else:\n",
    "        # do a best guess based on file extension\n",
    "        delimiter = \",\" if sample_sheet_config[\"path\"].endswith(\".csv\") else \"\\t\"\n",
    "    header = 0\n",
    "    # if \"header\" in sample_sheet_config:\n",
    "    #     header = sample_sheet_config[\"header\"]\n",
    "    # else:\n",
    "    #     # check if the first line starts with a #, if so lets assume it's a header otherwise assume there isn't one\n",
    "    #     with open(sample_sheet_config[\"path\"], \"r\") as f:\n",
    "    #         first_line = f.readline()\n",
    "    #         header = 0 if first_line.startswith(\"#\") else None\n",
    "    if \"columns\" in sample_sheet_config:\n",
    "        columns = sample_sheet_config[\n",
    "            \"columns\"\n",
    "        ]  # note the # for the first item needs to be stripped to compare to the columns\n",
    "    else:\n",
    "        columns = None  # implies all columns\n",
    "    try:\n",
    "        # note when we have a header the first column may begin with a #, so we need to remove it\n",
    "        df = pd.read_csv(\n",
    "            sample_sheet_config[\"path\"],\n",
    "            delimiter=delimiter,\n",
    "            header=header,\n",
    "            comment=None,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Error: Could not load sample sheet into dataframe, you have a problem with your sample sheet or the configuration.\"\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "    # Check the first header has a # in it, if so remove it for only that item\n",
    "    if df.columns[0].startswith(\"#\"):\n",
    "        df.columns = [col.lstrip(\"#\") for col in df.columns]\n",
    "    # Ensure the sample sheet has the correct columns\n",
    "    if columns is not None and not all([col in df.columns for col in columns]):\n",
    "        raise ValueError(\"Error: Sample sheet does not have the correct columns\")\n",
    "    # also drop columns which are not needed\n",
    "    if columns is not None:\n",
    "        df = df[columns]\n",
    "\n",
    "    # Clean the df of any extra rows that can be caused by empty lines in the sample sheet\n",
    "    df = df.dropna(how=\"all\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below are **not** tempalted and you should adjust this with your own code. It's included as an example of how to code some functions with associated tests and how to make it work on the command line. It is best to code by creating a new workbook and then importing the functions of this into that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def hello_world(name: str = \"Not given\") -> str:\n",
    "    return f\"Hello World! My name is {name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This here is a a test as part of fastcore.test, all fastcore tests will be automatically run when doing nbdev_test as well as through github actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.test_eq(\"Hello World! My name is Kim\", hello_world(\"Kim\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The @call_parse will, with the settings.ini entry way, automatically transform your function into a command line tool. Comments of the functions will appear for help messages and the initial docstring will appear in the help as well. You can also define defaults for the arguments and should define a typehint to control inputs. The function will likely have to resolve variables with ENV vars and config files. The recommended way to do this is to assume variables passed here are a higher priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from fastcore.script import call_parse\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def cli(\n",
    "    name: str,  # Your name\n",
    "    config_file: str = None,  # config file to set env vars from\n",
    "):\n",
    "    \"\"\"\n",
    "    This will print Hello World! with your name\n",
    "    \"\"\"\n",
    "    config = get_config(config_file)  # Set env vars and get config variables\n",
    "    if name is not None:\n",
    "        config[\"example\"][\"input\"][\"name\"] = name\n",
    "\n",
    "    print(hello_world(config[\"example\"][\"input\"][\"name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function with potentially variable input to confirm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World! My name is Kim\n"
     ]
    }
   ],
   "source": [
    "test.test_eq(\n",
    "    \"Hello World! My name is Kim\", hello_world(config[\"example\"][\"input\"][\"name\"])\n",
    ")\n",
    "test.test_eq(None, cli(\"Kim\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World! My name is Kim\n",
      "Hello World! My name is Lee\n"
     ]
    }
   ],
   "source": [
    "cli(config[\"example\"][\"input\"][\"name\"])\n",
    "cli(config[\"example\"][\"input\"][\"alternative_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import json\n",
    "import pandas as pd\n",
    "import json\n",
    "import yaml\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class DataFrame:\n",
    "    def __init__(self, data=None):\n",
    "        \"\"\"\n",
    "        Initialize the DataFrame object.\n",
    "        :param data: Optional initial data for the DataFrame.\n",
    "        \"\"\"\n",
    "        if data is not None:\n",
    "            self.df = pd.DataFrame(data)\n",
    "        else:\n",
    "            self.df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to deal with json imports and exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def import_nested_json_data(self, json_file_path):\n",
    "    \"\"\"\n",
    "    Import nested JSON data from a file and create headers by combining headers with underscores.\n",
    "    :param json_file_path: Path to the JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        nested_json = json.load(file)\n",
    "\n",
    "    def flatten_json(y):\n",
    "        out = {}        \n",
    "        def flatten(x, name=''):\n",
    "            if type(x) is dict:\n",
    "                for a in x:\n",
    "                    flatten(x[a], name + a + '£')\n",
    "            elif type(x) is list:\n",
    "                i = 1  # Start numbering from 1\n",
    "                for a in x:\n",
    "                    flatten(a, name + str(i) + '£')\n",
    "                    i += 1\n",
    "            else:\n",
    "                out[name[:-1]] = x\n",
    "\n",
    "        flatten(y)\n",
    "        return out\n",
    "\n",
    "    if isinstance(nested_json, list):\n",
    "        flat_data = [flatten_json(item) for item in nested_json]\n",
    "        self.df = pd.json_normalize(flat_data)\n",
    "    else:\n",
    "        flat_data = flatten_json(nested_json)\n",
    "        self.df = pd.json_normalize(flat_data)\n",
    "\n",
    "def export_nested_json_data(self, json_file_path):\n",
    "    \"\"\"\n",
    "    Export the DataFrame to a nested JSON file by unraveling headers with underscores.\n",
    "    :param json_file_path: Path to the JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    def unflatten_json(flat_dict):\n",
    "        out = {}\n",
    "\n",
    "        for key, value in flat_dict.items():\n",
    "            keys = key.split('£')\n",
    "            d = out\n",
    "            for k in keys[:-1]:\n",
    "                d = d.setdefault(k, {})\n",
    "            d[keys[-1]] = value\n",
    "        return out\n",
    "\n",
    "    nested_json_list = [unflatten_json(row) for row in self.df.to_dict(orient='records')]\n",
    "\n",
    "    def convert_to_list(d):\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, dict):\n",
    "                if all(k.isdigit() for k in value.keys()):\n",
    "                    d[key] = [v for k, v in sorted(value.items(), key=lambda item: int(item[0]))]\n",
    "                else:\n",
    "                    convert_to_list(value)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        convert_to_list(item)\n",
    "\n",
    "    def has_more_layers(d):\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, dict):\n",
    "                if any(k.isdigit() for k in value.keys()):\n",
    "                    return True\n",
    "                else:\n",
    "                    if has_more_layers(value):\n",
    "                        return True\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict) and has_more_layers(item):\n",
    "                        return True\n",
    "        return False\n",
    "    \n",
    "    for i, nested_json in enumerate(nested_json_list):\n",
    "        while has_more_layers(nested_json):\n",
    "            convert_to_list(nested_json)\n",
    "\n",
    "        if all(key.split('£')[0].isdigit() for key in nested_json.keys()):\n",
    "            nested_json_list[i] = [nested_json[str(i)] for i in range(1, len(nested_json) + 1)]\n",
    "\n",
    "    if len(nested_json_list) == 1:\n",
    "        nested_json_list = nested_json_list[0]\n",
    "\n",
    "    with open(json_file_path, 'w') as file:\n",
    "        json.dump(nested_json_list, file, indent=4)\n",
    "\n",
    "DataFrame.import_nested_json_data = import_nested_json_data\n",
    "DataFrame.export_nested_json_data = export_nested_json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to deal with xml imports and exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def import_nested_xml_data(self, xml_file_path):\n",
    "    \"\"\"\n",
    "    Import nested XML data from a file and create headers by combining headers with underscores.\n",
    "    :param xml_file_path: Path to the XML file.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    def flatten_xml(element, parent_name=''):\n",
    "        items = {}\n",
    "        for child in element:\n",
    "            child_name = f\"{parent_name}{element.tag}_{child.tag}_\"\n",
    "            if len(child):\n",
    "                items.update(flatten_xml(child, child_name))\n",
    "            else:\n",
    "                items[child_name[:-1]] = child.text\n",
    "        return items\n",
    "\n",
    "    flat_data = [flatten_xml(child) for child in root]\n",
    "    self.df = pd.json_normalize(flat_data)\n",
    "\n",
    "def export_nested_xml_data(self, xml_file_path):\n",
    "    root = ET.Element(\"Invetory\")\n",
    "    for _, row in self.df.iterrows():\n",
    "        tag_created = False\n",
    "        for col in self.df.columns:\n",
    "            tags = col.split('_')\n",
    "            for tag in tags[:-1]:\n",
    "                #print(tags[:-1])\n",
    "                if tag_created is False:\n",
    "                    new_tag = ET.SubElement(root, tag)\n",
    "                    tag_created = True\n",
    "                ET.SubElement(new_tag, tags[-1]).text = str(row[col])\n",
    "            \n",
    "\n",
    "    # Convert to string and pretty print using minidom\n",
    "    xml_str = ET.tostring(root, encoding='utf-8')\n",
    "    parsed_str = minidom.parseString(xml_str)\n",
    "    pretty_xml_str = parsed_str.toprettyxml(indent=\"  \")\n",
    "    \n",
    "    with open(xml_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(pretty_xml_str)\n",
    "\n",
    "DataFrame.import_nested_xml_data = import_nested_xml_data\n",
    "DataFrame.export_nested_xml_data = export_nested_xml_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definining base functions for DataFrame object\n",
    "import_data\n",
    "export_data\n",
    "rename_header\n",
    "filter_rows\n",
    "filter_columns\n",
    "show\n",
    "print_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def import_data(self, file_path, file_type='csv', add_header=0):\n",
    "    \"\"\"\n",
    "    Import data from a CSV, TSV, JSON, XML, or YAML file.\n",
    "    :param file_path: Path to the file.\n",
    "    :param file_type: Type of the file ('csv', 'tsv', 'json', 'xml', 'yaml').\n",
    "    :param delimiter: Delimiter used in the file (default is comma for CSV).\n",
    "    \"\"\"\n",
    "    # Check if add_header is a string and split it into a list\n",
    "    if isinstance(add_header, str):\n",
    "        if len(add_header) > 0:\n",
    "            add_header = add_header.replace(\" \", \"\").split(',')\n",
    "    #elif isinstance(add_header, list) and len(add_header > 0):\n",
    "\n",
    "\n",
    "    if file_type == 'csv':\n",
    "        self.df = pd.read_csv(file_path, delimiter=',', names=add_header if add_header else None)\n",
    "        #if isinstance(add_header, str):\n",
    "        if add_header:\n",
    "            if len(add_header) != len(self.df.columns):\n",
    "                raise ValueError(f\"Error: Number of new column names ({len(add_header)}) must match the number of columns in the DataFrame ({len(self.df.columns)}).\")\n",
    "            #self.df.columns = new_columns\n",
    "    elif file_type == 'tsv':\n",
    "        self.df = pd.read_csv(file_path, delimiter='\\t', names=add_header if add_header else None)\n",
    "        if add_header:\n",
    "            if len(add_header) != len(self.df.columns):\n",
    "                raise ValueError(f\"Error: Number of new column names ({len(add_header)}) must match the number of columns in the DataFrame ({len(self.df.columns)}).\")\n",
    "            #self.df.columns = new_columns\n",
    "    elif file_type == 'json':\n",
    "        self.import_nested_json_data(file_path)\n",
    "    elif file_type == 'yaml':\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "            self.df = pd.json_normalize(data)\n",
    "    elif file_type == 'xml':\n",
    "        self.import_nested_xml_data(file_path)\n",
    "    #    tree = ET.parse(file_path)\n",
    "    #    root = tree.getroot()\n",
    "    #    data = self._xml_to_dict(root)\n",
    "    #    self.df = pd.json_normalize(data)\n",
    "    #    self.df.columns = [col.replace('item.', '') for col in self.df.columns]\n",
    "    #else:\n",
    "    #    raise ValueError(\"Unsupported file type. Supported types: 'csv', 'tsv', 'json', 'xml', 'yaml'.\")\n",
    "\n",
    "def rename_header(self, new_columns):\n",
    "    \"\"\"\n",
    "    Rename columns in the DataFrame.\n",
    "    :param new_columns: Comma-separated string of new column names.\n",
    "    \"\"\"\n",
    "    new_columns_list = [col.strip() for col in new_columns.split(',')]\n",
    "    if len(new_columns_list) != len(self.df.columns):\n",
    "        print(\"Error: Number of new column names must match the number of columns in the DataFrame.\")\n",
    "        print(\"Current header:\", self.df.columns.tolist())\n",
    "    else:\n",
    "        self.df.columns = new_columns_list\n",
    "\n",
    "def filter_columns(self, columns):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame to include only specified columns.\n",
    "    :param columns: List of columns to include or list of boolean values.\n",
    "    \"\"\"\n",
    "    if all(isinstance(col, str) for col in columns):\n",
    "        # Case 1: Comma-separated string of column names\n",
    "        columns_list = [col.strip() for col in columns.split(',')]\n",
    "        if not all(col in self.df.columns for col in columns_list):\n",
    "            missing_cols = [col for col in columns_list if col not in self.df.columns]\n",
    "            raise ValueError(f\"Error: The following columns do not exist in the DataFrame: {missing_cols}\")\n",
    "        self.df = self.df[columns_list]\n",
    "    elif all(isinstance(col, bool) for col in columns):\n",
    "        # Case 2: List of boolean values\n",
    "        if len(columns) != len(self.df.columns):\n",
    "            raise ValueError(\"Error: Number of boolean values must match the number of columns in the DataFrame.\")\n",
    "        self.df = self.df.loc[:, columns]\n",
    "    else:\n",
    "        raise ValueError(\"Error: columns parameter must be a list of column names or a list of boolean values.\")\n",
    "\n",
    "def filter_rows(self, condition):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame to include only rows that meet the condition.\n",
    "    :param condition: List of integers (row indices starting with 1) or list of boolean values.\n",
    "    \"\"\"\n",
    "    if all(isinstance(cond, bool) for cond in condition):\n",
    "        # Case 1: List of boolean values\n",
    "        if len(condition) != len(self.df):\n",
    "            raise ValueError(\"Error: Number of boolean values must match the number of rows in the DataFrame.\")\n",
    "        self.df = self.df[condition]\n",
    "    elif all(isinstance(cond, int) for cond in condition):\n",
    "        # Case 2: List of integers (row indices starting with 1)\n",
    "        if any(cond < 1 or cond > len(self.df) for cond in condition):\n",
    "            raise ValueError(\"Error: One or more row indices are outside the scope of the DataFrame.\")\n",
    "        self.df = self.df.iloc[[cond - 1 for cond in condition]]\n",
    "    else:\n",
    "        raise ValueError(\"Error: condition parameter must be a list of integers or a list of boolean values.\")\n",
    "    # Renumber the rows starting from 1\n",
    "    self.df.index = range(1, len(self.df) + 1)\n",
    "\n",
    "def export_data(self, file_path, file_type='csv'):\n",
    "    \"\"\"\n",
    "    Export data to a CSV, TSV, JSON, YAML, or XML file.\n",
    "    :param file_path: Path to the file.\n",
    "    :param file_type: Type of the file ('csv', 'tsv', 'json', 'yaml', 'xml').\n",
    "    :param delimiter: Delimiter to use in the file (default is comma for CSV/TSV).\n",
    "    \"\"\"\n",
    "    if file_type == 'csv':\n",
    "        self.df.to_csv(file_path, index=False, sep=',')\n",
    "    elif file_type == 'tsv':\n",
    "        self.df.to_csv(file_path, index=False, sep='\\t')\n",
    "    elif file_type == 'json':\n",
    "        self.export_nested_json_data(file_path)\n",
    "    elif file_type == 'yaml':\n",
    "        with open(file_path, 'w') as f:\n",
    "            yaml.dump(self.df.to_dict(orient='records'), f, sort_keys=False)\n",
    "    elif file_type == 'xml':\n",
    "        self.export_nested_xml_data(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Supported types: 'csv', 'tsv', 'json', 'yaml', 'xml'.\")\n",
    "\n",
    "def print_header(self):\n",
    "    \"\"\"\n",
    "    Print the header of the DataFrame as a list.\n",
    "    \"\"\"\n",
    "    print(self.df.columns.tolist())\n",
    "\n",
    "def show(self):\n",
    "    \"\"\"\n",
    "    Display the DataFrame.\n",
    "    \"\"\"\n",
    "    print(self.df)\n",
    "\n",
    "DataFrame.import_data = import_data\n",
    "DataFrame.rename_header = rename_header\n",
    "DataFrame.filter_columns = filter_columns\n",
    "DataFrame.filter_rows = filter_rows\n",
    "DataFrame.export_data = export_data\n",
    "DataFrame.print_header = print_header\n",
    "DataFrame.show = show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SampleID', 'Species', 'Bracken', 'SampleIzD', 'Speczies', 'Brackenz', 'z']\n",
      "                                      SampleID      Species       Bracken  \\\n",
      "0                                         name  taxonomy_id  taxonomy_lvl   \n",
      "1                             Escherichia coli          562             S   \n",
      "2                         Escherichia albertii       208962             S   \n",
      "3                         Escherichia marmotae      1499973             S   \n",
      "4                       Escherichia fergusonii          564             S   \n",
      "5                        Citrobacter werkmanii        67827             S   \n",
      "6                         Citrobacter freundii          546             S   \n",
      "7                           Citrobacter koseri          545             S   \n",
      "8                        Citrobacter rodentium        67825             S   \n",
      "9                     Citrobacter amalonaticus        35703             S   \n",
      "10                Citrobacter sp. FDAARGOS_156      1702170             S   \n",
      "11                         Citrobacter farmeri        67824             S   \n",
      "12                         Salmonella enterica        28901             S   \n",
      "13                          Salmonella bongori        54736             S   \n",
      "14                        Shigella dysenteriae          622             S   \n",
      "15                           Shigella flexneri          623             S   \n",
      "16                       Klebsiella pneumoniae          573             S   \n",
      "17                        Klebsiella aerogenes          548             S   \n",
      "18                  Klebsiella quasipneumoniae      1463165             S   \n",
      "19                          Klebsiella oxytoca          571             S   \n",
      "20                    Klebsiella michiganensis      1134687             S   \n",
      "21                        Klebsiella variicola       244366             S   \n",
      "22                   Klebsiella quasivariicola      2026240             S   \n",
      "23                        Enterobacter cloacae          550             S   \n",
      "24                     Enterobacter hormaechei       158836             S   \n",
      "25                       Enterobacter asburiae        61645             S   \n",
      "26                   Enterobacter cancerogenus        69218             S   \n",
      "27            Enterobacter cloacae complex sp.      2027919             S   \n",
      "28                      Enterobacter sp. SA187      1914861             S   \n",
      "29                      Enterobacter sp. FY-07      1692238             S   \n",
      "30                     Enterobacter sp. R4-368      1166130             S   \n",
      "31                        Enterobacter sp. 638       399742             S   \n",
      "32         Enterobacteriaceae bacterium ENNIH1      2066051             S   \n",
      "33  Enterobacteriaceae bacterium strain FGI 57       693444             S   \n",
      "34                     Cronobacter dublinensis       413497             S   \n",
      "35                      Cronobacter muytjensii       413501             S   \n",
      "36                       Cronobacter sakazakii        28141             S   \n",
      "37                  Raoultella ornithinolytica        54291             S   \n",
      "38                       Raoultella planticola          575             S   \n",
      "39                           Kosakonia cowanii       208223             S   \n",
      "40                            Kosakonia oryzae       497725             S   \n",
      "41                          Kosakonia sacchari      1158459             S   \n",
      "42                         Lelliottia amnigena        61646             S   \n",
      "43                        Lelliottia sp. WB101      2153385             S   \n",
      "44                         Lelliottia jeotgali      1907578             S   \n",
      "45                          Shimwellia blattae          563             S   \n",
      "46                 [Enterobacter] lignolyticus      1334193             S   \n",
      "47                     Pluralibacter gergoviae        61647             S   \n",
      "48                              Cedecea neteri       158822             S   \n",
      "49                         Kluyvera intermedia        61648             S   \n",
      "50                     Gibbsiella quercinecans       929813             S   \n",
      "51                                Homo sapiens         9606             S   \n",
      "\n",
      "                SampleIzD     Speczies       Brackenz                     z  \n",
      "0   kraken_assigned_reads  added_reads  new_est_reads  fraction_total_reads  \n",
      "1                  923150      1185249        2108399               0.99226  \n",
      "2                    1329          191           1520               0.00072  \n",
      "3                     737           61            798               0.00038  \n",
      "4                     272           62            334               0.00016  \n",
      "5                      52           46             98               0.00005  \n",
      "6                      48          308            356               0.00017  \n",
      "7                     267           38            305               0.00014  \n",
      "8                     128           10            138               0.00006  \n",
      "9                      91          152            243               0.00011  \n",
      "10                     42           46             88               0.00004  \n",
      "11                     24           16             40               0.00002  \n",
      "12                   1160         5561           6721               0.00316  \n",
      "13                     60           32             92               0.00004  \n",
      "14                    812         1176           1988               0.00094  \n",
      "15                    410          701           1111               0.00052  \n",
      "16                    579          161            740               0.00035  \n",
      "17                    119            4            123               0.00006  \n",
      "18                     60           45            105               0.00005  \n",
      "19                     37            8             45               0.00002  \n",
      "20                     22          164            186               0.00009  \n",
      "21                     11           10             21               0.00001  \n",
      "22                     10            1             11               0.00001  \n",
      "23                    119          148            267               0.00013  \n",
      "24                     94           70            164               0.00008  \n",
      "25                     33            8             41               0.00002  \n",
      "26                     11            0             11               0.00001  \n",
      "27                     10            4             14               0.00001  \n",
      "28                     92            1             93               0.00004  \n",
      "29                     27            0             27               0.00001  \n",
      "30                     14            0             14               0.00001  \n",
      "31                     12            0             12               0.00001  \n",
      "32                     71           21             92               0.00004  \n",
      "33                     49            1             50               0.00002  \n",
      "34                     51           16             67               0.00003  \n",
      "35                     13            4             17               0.00001  \n",
      "36                     10           16             26               0.00001  \n",
      "37                     20           68             88               0.00004  \n",
      "38                     12            3             15               0.00001  \n",
      "39                     46            0             46               0.00002  \n",
      "40                     16           26             42               0.00002  \n",
      "41                     13            0             13               0.00001  \n",
      "42                     29            0             29               0.00001  \n",
      "43                     28            1             29               0.00001  \n",
      "44                     27            2             29               0.00001  \n",
      "45                     48            0             48               0.00002  \n",
      "46                     25            0             25               0.00001  \n",
      "47                     20            0             20               0.00001  \n",
      "48                     40            0             40               0.00002  \n",
      "49                     13            1             14               0.00001  \n",
      "50                     10            0             10               0.00000  \n",
      "51                     10            0             10               0.00000  \n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "#df = DataFrame()\n",
    "#df.import_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/sample.csv', file_type='csv')\n",
    "#df.show()\n",
    "#df.import_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/sample.tsv', file_type='tsv')\n",
    "#df.show()\n",
    "#df.import_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/sample.json', file_type='json')\n",
    "#df.show()\n",
    "#df.import_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/sample2.xml', file_type='xml')\n",
    "#df.show()\n",
    "#df.import_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/nested_example.json', file_type='json')\n",
    "#df.show()\n",
    "#df.print_header()\n",
    "#df.rename_header(['column1', 'column2', 'column3'])\n",
    "#df.print_header()\n",
    "#df.show()\n",
    "#df.filter_columns(['column1', 'column2'])\n",
    "#df.show()\n",
    "#df.filter_columns([True, False])\n",
    "#df.show()\n",
    "#df.filter_rows([1, 3])\n",
    "#df.show()\n",
    "#df.filter_rows([True, False])\n",
    "#df.show()\n",
    "\n",
    "#df.export_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/export.json', file_type='xml')\n",
    "\n",
    "dfz = DataFrame()\n",
    "#dfz.import_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/mlst_report.tabular', file_type='tsv', header_exists=0, add_header=\"SampleID, Species, ST, 1, 2, 3, 4, 5, 6, 7\")\n",
    "#dfz.filter_columns(\"SampleID, Species, ST\")\n",
    "#dfz.import_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/bracken.tabular', file_type='tsv', add_header=[\"SampleID\", \"Species\", \"Bracken\", \"SampleIzD\", \"Speczies\", \"Brackenz\", \"z\"])\n",
    "dfz.import_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/bracken.tabular', file_type='tsv', add_header=\"SampleID, Species, Bracken, SampleIzD, Speczies, Brackenz, z\")\n",
    "#dfz.import_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/bracken.tabular', file_type='tsv')\n",
    "#dfz.import_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/TestSample2.json', file_type='json')\n",
    "dfz.show()\n",
    "# Example usage:\n",
    "#dfz.export_data('/Users/B246654/vscode_storage/ssi-dk/bifrost_bridge/test_data/t3.json', file_type='json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status curently:\n",
    "full csv, tsv, json support\n",
    "partial xml and yaml support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# This is included at the end to ensure when you run through your notebook the code is also transferred to the associated python package\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
