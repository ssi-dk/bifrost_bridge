{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp core\n",
    "# This will create a package named bifrost_bridge/core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev\n",
    "from nbdev.showdoc import *  # ignore this Pylance warning in favor of following nbdev docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For help with the Markdown language, see [this guide](https://www.markdownguide.org/basic-syntax/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global static vars\n",
    "These are used to modify the template for individual use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# Need the bifrost_bridge for a few functions, this can be considered a static var\n",
    "\n",
    "import importlib\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "PACKAGE_NAME: str = (\n",
    "    \"bifrost_bridge\"  # Make sure to adjust this to your package name\n",
    ")\n",
    "DEV_MODE: bool = (\n",
    "    False  # set below to override, as this is in an export block it'll be exported while the dev mode section is not\n",
    ")\n",
    "\n",
    "PACKAGE_DIR = None\n",
    "try:\n",
    "    spec = importlib.util.find_spec(PACKAGE_NAME)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    PACKAGE_DIR = os.path.dirname(module.__file__)\n",
    "except ImportError:\n",
    "    DEV_MODE = True\n",
    "except AttributeError:\n",
    "    DEV_MODE = True\n",
    "PROJECT_DIR = os.getcwd()  # override value in dev mode\n",
    "if PROJECT_DIR.endswith('nbs'):\n",
    "    DEV_MODE=True\n",
    "    PROJECT_DIR=os.path.split(PROJECT_DIR)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev mode\n",
    "If you're developing this versus running this, you'll have access to slightly different things. Notable the nbdev functions are only for development and not for runtime. This matters for items such as the config. So we need to detect if you are in dev mode or not and the code has to adjust accordingly. Notice that this section is not exported so will only work in the notebook and not in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section uses nbdev functions so should not be exported as it's for dev purposes\n",
    "import os\n",
    "\n",
    "if DEV_MODE:\n",
    "    PACKAGE_DIR = nbdev.config.get_config(cfg_name=\"settings.ini\", path=os.getcwd())[\n",
    "        \"lib_path\"\n",
    "    ]  # the library is the package of course\n",
    "    PROJECT_DIR = nbdev.config.get_config(\n",
    "        cfg_name=\"settings.ini\", path=os.getcwd()\n",
    "    ).config_path  # the default location of nbdev config file (settings.ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    " A module which contains common functions to be used by other modules. Those that exist in the template are meant to be common functions we can use against multiple packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#|hide\n",
    "\n",
    "Notebook blocks starting with #|hide are not shown in the documentation and not exported to the python package. Blocks with #|export are exported to the python package. Blocks with neither are shown to the documentation but not exported to the python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Currently all libraries included are listed at the top and calls to them are also made in the block of code that uses them. This is for readability and the performance hit of the import is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# standard libs\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Common to template\n",
    "# add into settings.ini, requirements, package name is python-dotenv, for conda build ensure `conda config --add channels conda-forge`\n",
    "import dotenv  # for loading config from .env files, https://pypi.org/project/python-dotenv/\n",
    "import envyaml  # Allows to loads env vars into a yaml file, https://github.com/thesimj/envyaml\n",
    "import fastcore  # To add functionality related to nbdev development, https://github.com/fastai/fastcore/\n",
    "import pandas  # For sample sheet manipulation\n",
    "from fastcore import (\n",
    "    test,\n",
    ")\n",
    "from fastcore.script import (\n",
    "    call_parse,\n",
    ")  # for @call_parse, https://fastcore.fast.ai/script\n",
    "\n",
    "# Project specific libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Our config file holds all program and user specific variables. This is a good practice to follow as it allows us to easily change variables without having to change code. It also allows us to easily change variables based on the environment we are running in. For example, we may want to run a program in a test environment with a different database than we would in production. This is also a good practice to follow as it allows us to easily change variables without having to change code. It also allows us to easily change variables based on the environment we are running in. For example, we may want to run a program in a test environment with a different database than we would in production.\n",
    "\n",
    "Configuration is templated to rely on environment (ENV) variables. A default ENV config is provided in `./config/config.default.env` and more advanced data structures are supported in `./config/config.default.yaml`. The `.yaml` file is meant to represent what your program actually works with and the `.env` file options the user can change at run time.\n",
    "\n",
    "Make sure you know the priority of variables and check on them when debugging your code. Also ensure that your yaml file is referenced appropriately in the `.env` file. \n",
    "\n",
    "When in use there's an expectation you'll have multiple config files for different use cases e.g. development, production environment for different paths, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set env variables\n",
    "A helper function for getting your config values, this will set the environment variables with the provided `.env` values. If you're missing values it'll ensure they're loaded in with the defaults file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import importlib\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "def set_env_variables(config_path: str, overide_env_vars: bool = True) -> bool:\n",
    "    # Load dot env sets environmental values from a file, if the value already exists it will not be overwritten unless override is set to True.\n",
    "    # If we have multiple .env files then we need to apply the one which we want to take precedence last with overide.\n",
    "\n",
    "    # Order of precedence: .env file > environment variables > default values\n",
    "    # When developing, making a change to the config will not be reflected until the environment is restarted\n",
    "\n",
    "    # Set the env vars first, this is needed for the card.yaml to replace ENV variables\n",
    "    # NOTE: You need to adjust PROJECT_NAME to your package name for this to work, the exception is only for dev purposes\n",
    "    # This here checks if your package is installed, such as through pypi or through pip install -e  [.dev] for development. If it is then it'll go there and use the config files there as your default values.\n",
    "    try:\n",
    "        dotenv.load_dotenv(\n",
    "            f\"{PACKAGE_DIR}/config/config.default.env\", override=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {PACKAGE_DIR}/config/config.default.env does not exist\")\n",
    "        return False\n",
    "\n",
    "    # 2. set values from file:\n",
    "    if os.path.isfile(config_path):\n",
    "        dotenv.load_dotenv(config_path, override=overide_env_vars)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get config\n",
    "\n",
    "When you run this function, assuming things are set up properly, you end up with a dict that matches your `.yaml` file. This file will have all the inputs for the package and settings of your program.\n",
    "\n",
    "To do this it will use a `.env` config file, which has an associated yaml file defined with `CORE_YAML_CONFIG_FILE` in the `.env` file. And then use the `.env` file to load values into the associated `.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import importlib\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "def get_config(config_path: str = None, overide_env_vars: bool = True) -> dict:\n",
    "    if config_path is None:\n",
    "        config_path = \"\"\n",
    "    # First sets environment with variables from config_path, then uses those variables to fill in appropriate values in the config.yaml file, the yaml file is then returned as a dict\n",
    "    # If you want user env variables to take precedence over the config.yaml file then set overide_env_vars to False\n",
    "    set_env_variables(config_path, overide_env_vars)\n",
    "   \n",
    "    config: dict = envyaml.EnvYAML(\n",
    "        os.environ.get(\n",
    "            \"CORE_YAML_CONFIG_FILE\", f\"{PACKAGE_DIR}/config/config.default.yaml\"\n",
    "        ),\n",
    "        strict=False,\n",
    "    ).export()\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "All the user input variables and machine adjustable variables should be in your config, which is a dict. Reference config.default.yaml for how to access your variables. Also note that with python dicts you can use `dict_variable.get(\"variable\", default_value)` to ensure that you don't get a key error if the variable is not set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# create a os.PathLike object\n",
    "config = get_config(os.environ.get(\"CORE_CONFIG_FILE\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show project env vars\n",
    "A helper function intended to only be used with debugging. It shows all your project specific environmental variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def show_project_env_vars(config: dict) -> None:\n",
    "    # Prints out all the project environment variables\n",
    "    # This is useful for debugging and seeing what is being set\n",
    "    for k, v in config.items():\n",
    "        # If ENV var starts with PROJECTNAME_ then print\n",
    "        if k.startswith(config[\"CORE_PROJECT_VARIABLE_PREFIX\"]):\n",
    "            print(f\"{k}={v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIFROST_BRIDGE_INPUT_DIR=./input\n",
      "BIFROST_BRIDGE_OUTPUT_DIR=./output\n",
      "BIFROST_BRIDGE_OUTPUT_FILE=./output/output.txt\n",
      "BIFROST_BRIDGE_USER_INPUT_NAME=Kim\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "show_project_env_vars(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_samplesheet\n",
    "This function is to unify the way we work with sample_sheet's which is for us a file with a table of values, typically samples for batch processing. We want to approach doing it this way so all programs have batch processing in mind and working with the same data structure.\n",
    "\n",
    "To make use of it we have a small sample_sheet yaml object which looks like\n",
    "    \n",
    "```yaml\n",
    "sample_sheet:\n",
    "    path: path/to/sample_sheet.tsv\n",
    "    delimiter: '\\t' # Optional, will assume , for csv and \\t otherwises\n",
    "    header: 0 # Optional, 0 indicates first row is header, None indicates no header\n",
    "    columns: ['column1', 'column2', 'column3'] # Optional, if not provided all columns will be used\n",
    "```\n",
    "\n",
    "Make sure to add that to your relevant section in your config (can be multiple times if you're working with different sheets or different columns), then call the function on this object and it'll either mention somethings wrong or return a pandas dataframe with the columns of interest.\n",
    "\n",
    "This is an example of a common sample_sheet we work with. We will ingest the hash at the beginning so it doesn't affect column naming. Extra empty rows at the end are also stripped.\n",
    "```tsv\n",
    "#sample_id\tfile_path\tmetadata1\tmetadata2\n",
    "Sample1\t/path/to/sample1.fasta\tvalue1\toption1\n",
    "Sample2\t/path/to/sample2.fasta\tvalue2\toption2\n",
    "Sample3\t/path/to/sample3.fasta\tvalue3\toption1\n",
    "Sample4\t/path/to/sample4.fasta\tvalue1\toption2\n",
    "Sample5\t/path/to/sample5.fasta\tvalue2\toption1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_samplesheet(sample_sheet_config: dict) -> pd.DataFrame:\n",
    "    # Load the sample sheet into a pandas dataframe\n",
    "    # If columns is not None then it will only load those columns\n",
    "    # If the sample sheet is a csv then it will load it as a csv, otherwise it will assume it's a tsv\n",
    "\n",
    "    # Expected sample_sheet_config:\n",
    "    # sample_sheet:\n",
    "    #   path: path/to/sample_sheet.tsv\n",
    "    #   delimiter: '\\t' # Optional, will assume , for csv and \\t otherwises\n",
    "    #   header: 0 # Optional, 0 indicates first row is header, None indicates no header\n",
    "    #   columns: ['column1', 'column2', 'column3'] # Optional, if not provided all columns will be used\n",
    "\n",
    "    # Example sample sheet:\n",
    "    # #sample_id\tfile_path\tmetadata1\tmetadata2\n",
    "    # Sample1\t/path/to/sample1.fasta\tvalue1\toption1\n",
    "    # Sample2\t/path/to/sample2.fasta\tvalue2\toption2\n",
    "    # Sample3\t/path/to/sample3.fasta\tvalue3\toption1\n",
    "    # Sample4\t/path/to/sample4.fasta\tvalue1\toption2\n",
    "    # Sample5\t/path/to/sample5.fasta\tvalue2\toption1\n",
    "\n",
    "    # This function should also handle ensuring the sample sheet is in the correct format, such as ensuring the columns are correct and that the sample names are unique.\n",
    "    if not os.path.isfile(sample_sheet_config[\"path\"]):\n",
    "        raise FileNotFoundError(f\"File {sample_sheet_config['path']} does not exist\")\n",
    "    if \"delimiter\" in sample_sheet_config:\n",
    "        delimiter = sample_sheet_config[\"delimiter\"]\n",
    "    else:\n",
    "        # do a best guess based on file extension\n",
    "        delimiter = \",\" if sample_sheet_config[\"path\"].endswith(\".csv\") else \"\\t\"\n",
    "    header = 0\n",
    "    # if \"header\" in sample_sheet_config:\n",
    "    #     header = sample_sheet_config[\"header\"]\n",
    "    # else:\n",
    "    #     # check if the first line starts with a #, if so lets assume it's a header otherwise assume there isn't one\n",
    "    #     with open(sample_sheet_config[\"path\"], \"r\") as f:\n",
    "    #         first_line = f.readline()\n",
    "    #         header = 0 if first_line.startswith(\"#\") else None\n",
    "    if \"columns\" in sample_sheet_config:\n",
    "        columns = sample_sheet_config[\n",
    "            \"columns\"\n",
    "        ]  # note the # for the first item needs to be stripped to compare to the columns\n",
    "    else:\n",
    "        columns = None  # implies all columns\n",
    "    try:\n",
    "        # note when we have a header the first column may begin with a #, so we need to remove it\n",
    "        df = pd.read_csv(\n",
    "            sample_sheet_config[\"path\"],\n",
    "            delimiter=delimiter,\n",
    "            header=header,\n",
    "            comment=None,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Error: Could not load sample sheet into dataframe, you have a problem with your sample sheet or the configuration.\"\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "    # Check the first header has a # in it, if so remove it for only that item\n",
    "    if df.columns[0].startswith(\"#\"):\n",
    "        df.columns = [col.lstrip(\"#\") for col in df.columns]\n",
    "    # Ensure the sample sheet has the correct columns\n",
    "    if columns is not None and not all([col in df.columns for col in columns]):\n",
    "        raise ValueError(\"Error: Sample sheet does not have the correct columns\")\n",
    "    # also drop columns which are not needed\n",
    "    if columns is not None:\n",
    "        df = df[columns]\n",
    "\n",
    "    # Clean the df of any extra rows that can be caused by empty lines in the sample sheet\n",
    "    df = df.dropna(how=\"all\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below are **not** tempalted and you should adjust this with your own code. It's included as an example of how to code some functions with associated tests and how to make it work on the command line. It is best to code by creating a new workbook and then importing the functions of this into that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def hello_world(name: str = \"Not given\") -> str:\n",
    "    return f\"Hello World! My name is {name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This here is a a test as part of fastcore.test, all fastcore tests will be automatically run when doing nbdev_test as well as through github actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.test_eq(\"Hello World! My name is Kim\", hello_world(\"Kim\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The @call_parse will, with the settings.ini entry way, automatically transform your function into a command line tool. Comments of the functions will appear for help messages and the initial docstring will appear in the help as well. You can also define defaults for the arguments and should define a typehint to control inputs. The function will likely have to resolve variables with ENV vars and config files. The recommended way to do this is to assume variables passed here are a higher priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from fastcore.script import call_parse\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def cli(\n",
    "    name: str,  # Your name\n",
    "    config_file: str = None,  # config file to set env vars from\n",
    "):\n",
    "    \"\"\"\n",
    "    This will print Hello World! with your name\n",
    "    \"\"\"\n",
    "    config = get_config(config_file)  # Set env vars and get config variables\n",
    "    if name is not None:\n",
    "        config[\"example\"][\"input\"][\"name\"] = name\n",
    "\n",
    "    print(hello_world(config[\"example\"][\"input\"][\"name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function with potentially variable input to confirm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World! My name is Kim\n"
     ]
    }
   ],
   "source": [
    "test.test_eq(\n",
    "    \"Hello World! My name is Kim\", hello_world(config[\"example\"][\"input\"][\"name\"])\n",
    ")\n",
    "test.test_eq(None, cli(\"Kim\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World! My name is Kim\n",
      "Hello World! My name is Lee\n"
     ]
    }
   ],
   "source": [
    "cli(config[\"example\"][\"input\"][\"name\"])\n",
    "cli(config[\"example\"][\"input\"][\"alternative_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import json\n",
    "import pandas as pd\n",
    "import json\n",
    "import yaml\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class DataFrame:\n",
    "    def __init__(self, data=None):\n",
    "        \"\"\"\n",
    "        Initialize the DataFrame object.\n",
    "        :param data: Optional initial data for the DataFrame.\n",
    "        \"\"\"\n",
    "        if data is not None:\n",
    "            self.df = pd.DataFrame(data)\n",
    "        else:\n",
    "            self.df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to deal with json imports and exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def import_nested_json_data(self, json_file_path):\n",
    "    \"\"\"\n",
    "    Import nested JSON data from a file and create headers by combining headers with underscores.\n",
    "    :param json_file_path: Path to the JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        nested_json = json.load(file)\n",
    "\n",
    "    def flatten_json(y):\n",
    "        out = {}        \n",
    "        def flatten(x, name=''):\n",
    "            if type(x) is dict:\n",
    "                for a in x:\n",
    "                    flatten(x[a], name + a + '£')\n",
    "            elif type(x) is list:\n",
    "                i = 1  # Start numbering from 1\n",
    "                for a in x:\n",
    "                    flatten(a, name + str(i) + '£')\n",
    "                    i += 1\n",
    "            else:\n",
    "                out[name[:-1]] = x\n",
    "\n",
    "        flatten(y)\n",
    "        return out\n",
    "\n",
    "    if isinstance(nested_json, list):\n",
    "        flat_data = [flatten_json(item) for item in nested_json]\n",
    "        self.df = pd.json_normalize(flat_data)\n",
    "    else:\n",
    "        flat_data = flatten_json(nested_json)\n",
    "        self.df = pd.json_normalize(flat_data)\n",
    "\n",
    "def export_nested_json_data(self, json_file_path):\n",
    "    \"\"\"\n",
    "    Export the DataFrame to a nested JSON file by unraveling headers with underscores.\n",
    "    :param json_file_path: Path to the JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    def unflatten_json(flat_dict):\n",
    "        out = {}\n",
    "\n",
    "        for key, value in flat_dict.items():\n",
    "            keys = key.split('£')\n",
    "            d = out\n",
    "            for k in keys[:-1]:\n",
    "                d = d.setdefault(k, {})\n",
    "            d[keys[-1]] = value\n",
    "        return out\n",
    "\n",
    "    nested_json_list = [unflatten_json(row) for row in self.df.to_dict(orient='records')]\n",
    "\n",
    "    def convert_to_list(d):\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, dict):\n",
    "                if all(k.isdigit() for k in value.keys()):\n",
    "                    d[key] = [v for k, v in sorted(value.items(), key=lambda item: int(item[0]))]\n",
    "                else:\n",
    "                    convert_to_list(value)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        convert_to_list(item)\n",
    "\n",
    "    def has_more_layers(d):\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, dict):\n",
    "                if any(k.isdigit() for k in value.keys()):\n",
    "                    return True\n",
    "                else:\n",
    "                    if has_more_layers(value):\n",
    "                        return True\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict) and has_more_layers(item):\n",
    "                        return True\n",
    "        return False\n",
    "    \n",
    "    for i, nested_json in enumerate(nested_json_list):\n",
    "        while has_more_layers(nested_json):\n",
    "            convert_to_list(nested_json)\n",
    "\n",
    "        if all(key.split('£')[0].isdigit() for key in nested_json.keys()):\n",
    "            nested_json_list[i] = [nested_json[str(i)] for i in range(1, len(nested_json) + 1)]\n",
    "\n",
    "    if len(nested_json_list) == 1:\n",
    "        nested_json_list = nested_json_list[0]\n",
    "\n",
    "    with open(json_file_path, 'w') as file:\n",
    "        json.dump(nested_json_list, file, indent=4)\n",
    "\n",
    "DataFrame.import_nested_json_data = import_nested_json_data\n",
    "DataFrame.export_nested_json_data = export_nested_json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to deal with xml imports and exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def import_nested_xml_data(self, xml_file_path):\n",
    "    \"\"\"\n",
    "    Import nested XML data from a file and create headers by combining headers with underscores.\n",
    "    :param xml_file_path: Path to the XML file.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    def flatten_xml(element, parent_name=''):\n",
    "        items = {}\n",
    "        for child in element:\n",
    "            child_name = f\"{parent_name}{element.tag}_{child.tag}_\"\n",
    "            if len(child):\n",
    "                items.update(flatten_xml(child, child_name))\n",
    "            else:\n",
    "                items[child_name[:-1]] = child.text\n",
    "        return items\n",
    "\n",
    "    flat_data = [flatten_xml(child) for child in root]\n",
    "    self.df = pd.json_normalize(flat_data)\n",
    "\n",
    "def export_nested_xml_data(self, xml_file_path):\n",
    "    root = ET.Element(\"Invetory\")\n",
    "    for _, row in self.df.iterrows():\n",
    "        tag_created = False\n",
    "        for col in self.df.columns:\n",
    "            tags = col.split('_')\n",
    "            for tag in tags[:-1]:\n",
    "                #print(tags[:-1])\n",
    "                if tag_created is False:\n",
    "                    new_tag = ET.SubElement(root, tag)\n",
    "                    tag_created = True\n",
    "                ET.SubElement(new_tag, tags[-1]).text = str(row[col])\n",
    "            \n",
    "\n",
    "    # Convert to string and pretty print using minidom\n",
    "    xml_str = ET.tostring(root, encoding='utf-8')\n",
    "    parsed_str = minidom.parseString(xml_str)\n",
    "    pretty_xml_str = parsed_str.toprettyxml(indent=\"  \")\n",
    "    \n",
    "    with open(xml_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(pretty_xml_str)\n",
    "\n",
    "DataFrame.import_nested_xml_data = import_nested_xml_data\n",
    "DataFrame.export_nested_xml_data = export_nested_xml_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definining base functions for DataFrame object\n",
    "import_data\n",
    "export_data\n",
    "rename_header\n",
    "filter_rows\n",
    "filter_columns\n",
    "show\n",
    "print_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def import_data(self, file_path, file_type='csv', add_header=''):\n",
    "    \"\"\"\n",
    "    Import data from a CSV, TSV, JSON, XML, or YAML file.\n",
    "    :param file_path: Path to the file.\n",
    "    :param file_type: Type of the file ('csv', 'tsv', 'json', 'xml', 'yaml').\n",
    "    :param delimiter: Delimiter used in the file (default is comma for CSV).\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if add_header is a string and split it into a list\n",
    "    if isinstance(add_header, str):\n",
    "        if len(add_header) > 0:\n",
    "            add_header = add_header.replace(\" \", \"\").split(',')\n",
    "\n",
    "    if file_type == 'csv':\n",
    "        self.df = pd.read_csv(file_path, delimiter=',', header=None if add_header else 0, index_col=False)\n",
    "        if add_header:\n",
    "            if len(add_header) != len(self.df.columns):\n",
    "                raise ValueError(f\"Error: Number of new column names ({len(add_header)}) must match the number of columns in the DataFrame ({len(self.df.columns)}).\")\n",
    "            else:\n",
    "                self.df.columns = add_header\n",
    "\n",
    "    elif file_type == 'tsv':\n",
    "        self.df = pd.read_csv(file_path, delimiter='\\t', header=None if add_header else 0, index_col=False)\n",
    "        if add_header:\n",
    "            if len(add_header) != len(self.df.columns):\n",
    "                raise ValueError(f\"Error: Number of new column names ({len(add_header)}) must match the number of columns in the DataFrame ({len(self.df.columns)}).\")\n",
    "            else:\n",
    "                self.df.columns = add_header\n",
    "\n",
    "    elif file_type == 'json':\n",
    "        self.import_nested_json_data(file_path)\n",
    "    elif file_type == 'yaml':\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "            self.df = pd.json_normalize(data)\n",
    "    elif file_type == 'xml':\n",
    "        self.import_nested_xml_data(file_path)\n",
    "    #    tree = ET.parse(file_path)\n",
    "    #    root = tree.getroot()\n",
    "    #    data = self._xml_to_dict(root)\n",
    "    #    self.df = pd.json_normalize(data)\n",
    "    #    self.df.columns = [col.replace('item.', '') for col in self.df.columns]\n",
    "    #else:\n",
    "    #    raise ValueError(\"Unsupported file type. Supported types: 'csv', 'tsv', 'json', 'xml', 'yaml'.\")\n",
    "\n",
    "def rename_header(self, new_columns):\n",
    "    \"\"\"\n",
    "    Rename columns in the DataFrame.\n",
    "    :param new_columns: Comma-separated string of new column names.\n",
    "    \"\"\"\n",
    "    new_columns_list = [col.strip() for col in new_columns.split(',')]\n",
    "    if len(new_columns_list) != len(self.df.columns):\n",
    "        print(\"Error: Number of new column names must match the number of columns in the DataFrame.\")\n",
    "        print(\"Current header:\", self.df.columns.tolist())\n",
    "    else:\n",
    "        self.df.columns = new_columns_list\n",
    "\n",
    "def filter_columns(self, columns):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame to include only specified columns.\n",
    "    :param columns: List of columns to include or list of boolean values.\n",
    "    \"\"\"\n",
    "    if all(isinstance(col, str) for col in columns):\n",
    "        # Case 1: Comma-separated string of column names\n",
    "        columns_list = [col.strip() for col in columns.split(',')]\n",
    "        if not all(col in self.df.columns for col in columns_list):\n",
    "            missing_cols = [col for col in columns_list if col not in self.df.columns]\n",
    "            raise ValueError(f\"Error: The following columns do not exist in the DataFrame: {missing_cols}\")\n",
    "        self.df = self.df[columns_list]\n",
    "    elif all(isinstance(col, bool) for col in columns):\n",
    "        # Case 2: List of boolean values\n",
    "        if len(columns) != len(self.df.columns):\n",
    "            raise ValueError(\"Error: Number of boolean values must match the number of columns in the DataFrame.\")\n",
    "        self.df = self.df.loc[:, columns]\n",
    "    else:\n",
    "        raise ValueError(\"Error: columns parameter must be a list of column names or a list of boolean values.\")\n",
    "\n",
    "def filter_rows(self, condition):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame to include only rows that meet the condition.\n",
    "    :param condition: List of integers (row indices starting with 1) or list of boolean values.\n",
    "    \"\"\"\n",
    "    if all(isinstance(cond, bool) for cond in condition):\n",
    "        # Case 1: List of boolean values\n",
    "        if len(condition) != len(self.df):\n",
    "            raise ValueError(\"Error: Number of boolean values must match the number of rows in the DataFrame.\")\n",
    "        self.df = self.df[condition]\n",
    "    elif all(isinstance(cond, int) for cond in condition):\n",
    "        # Case 2: List of integers (row indices starting with 1)\n",
    "        if any(cond < 1 or cond > len(self.df) for cond in condition):\n",
    "            raise ValueError(\"Error: One or more row indices are outside the scope of the DataFrame.\")\n",
    "        self.df = self.df.iloc[[cond - 1 for cond in condition]]\n",
    "    else:\n",
    "        raise ValueError(\"Error: condition parameter must be a list of integers or a list of boolean values.\")\n",
    "    # Renumber the rows starting from 1\n",
    "    self.df.index = range(1, len(self.df) + 1)\n",
    "\n",
    "def export_data(self, file_path, file_type='csv'):\n",
    "    \"\"\"\n",
    "    Export data to a CSV, TSV, JSON, YAML, or XML file.\n",
    "    :param file_path: Path to the file.\n",
    "    :param file_type: Type of the file ('csv', 'tsv', 'json', 'yaml', 'xml').\n",
    "    :param delimiter: Delimiter to use in the file (default is comma for CSV/TSV).\n",
    "    \"\"\"\n",
    "    if file_type == 'csv':\n",
    "        self.df.to_csv(file_path, index=False, sep=',')\n",
    "    elif file_type == 'tsv':\n",
    "        self.df.to_csv(file_path, index=False, sep='\\t')\n",
    "    elif file_type == 'json':\n",
    "        self.export_nested_json_data(file_path)\n",
    "    elif file_type == 'yaml':\n",
    "        with open(file_path, 'w') as f:\n",
    "            yaml.dump(self.df.to_dict(orient='records'), f, sort_keys=False)\n",
    "    elif file_type == 'xml':\n",
    "        self.export_nested_xml_data(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Supported types: 'csv', 'tsv', 'json', 'yaml', 'xml'.\")\n",
    "\n",
    "def print_header(self):\n",
    "    \"\"\"\n",
    "    Print the header of the DataFrame as a list.\n",
    "    \"\"\"\n",
    "    print(self.df.columns.tolist())\n",
    "\n",
    "def show(self):\n",
    "    \"\"\"\n",
    "    Display the DataFrame.\n",
    "    \"\"\"\n",
    "    print(self.df)\n",
    "\n",
    "DataFrame.import_data = import_data\n",
    "DataFrame.rename_header = rename_header\n",
    "DataFrame.filter_columns = filter_columns\n",
    "DataFrame.filter_rows = filter_rows\n",
    "DataFrame.export_data = export_data\n",
    "DataFrame.print_header = print_header\n",
    "DataFrame.show = show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_data/bracken.tabular'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2145], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m dfz \u001b[38;5;241m=\u001b[39m DataFrame()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#dfz.import_data('test_data/mlst_report.tabular', file_type='tsv', header_exists=0, add_header=\"SampleID, Species, ST, 1, 2, 3, 4, 5, 6, 7\")\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#dfz.filter_columns(\"SampleID, Species, ST\")\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#dfz.import_data('test_data/bracken.tabular', file_type='tsv', add_header=[\"SampleID\", \"Species\", \"Bracken\", \"SampleIzD\", \"Speczies\", \"Brackenz\", \"z\"])\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mdfz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_data/bracken.tabular\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSampleID, Species, Bracken, SampleIzD, Speczies, Brackenz, z\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#dfz.import_data('test_data/bracken.tabular', file_type='tsv')\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#dfz.import_data('test_data/bracken.tabular', file_type='tsv')\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#dfz.import_data('test_data/TestSample2.json', file_type='json')\u001b[39;00m\n\u001b[1;32m     36\u001b[0m dfz\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[2144], line 24\u001b[0m, in \u001b[0;36mimport_data\u001b[0;34m(self, file_path, file_type, add_header)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m add_header\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madd_header\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m add_header:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(add_header) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "File \u001b[0;32m~/vscode_storage/ssi-dk/bifrost_bridge/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscode_storage/ssi-dk/bifrost_bridge/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/vscode_storage/ssi-dk/bifrost_bridge/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscode_storage/ssi-dk/bifrost_bridge/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/vscode_storage/ssi-dk/bifrost_bridge/.venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_data/bracken.tabular'"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "os.chdir(core.PROJECT_DIR)\n",
    "#df = DataFrame()\n",
    "#df.import_data('test_data/sample.csv', file_type='csv')\n",
    "#df.show()\n",
    "#df.import_data('test_data/sample.tsv', file_type='tsv')\n",
    "#df.show()\n",
    "#df.import_data('test_data/sample.json', file_type='json')\n",
    "#df.show()\n",
    "#df.import_data('test_data/sample2.xml', file_type='xml')\n",
    "#df.show()\n",
    "#df.import_data('test_data/nested_example.json', file_type='json')\n",
    "#df.show()\n",
    "#df.print_header()\n",
    "#df.rename_header(['column1', 'column2', 'column3'])\n",
    "#df.print_header()\n",
    "#df.show()\n",
    "#df.filter_columns(['column1', 'column2'])\n",
    "#df.show()\n",
    "#df.filter_columns([True, False])\n",
    "#df.show()\n",
    "#df.filter_rows([1, 3])\n",
    "#df.show()\n",
    "#df.filter_rows([True, False])\n",
    "#df.show()\n",
    "\n",
    "#df.export_data('test_data/export.json', file_type='xml')\n",
    "\n",
    "dfz = DataFrame()\n",
    "#dfz.import_data('test_data/mlst_report.tabular', file_type='tsv', header_exists=0, add_header=\"SampleID, Species, ST, 1, 2, 3, 4, 5, 6, 7\")\n",
    "#dfz.filter_columns(\"SampleID, Species, ST\")\n",
    "#dfz.import_data('test_data/bracken.tabular', file_type='tsv', add_header=[\"SampleID\", \"Species\", \"Bracken\", \"SampleIzD\", \"Speczies\", \"Brackenz\", \"z\"])\n",
    "dfz.import_data('test_data/bracken.tabular', file_type='tsv', add_header=\"SampleID, Species, Bracken, SampleIzD, Speczies, Brackenz, z\")\n",
    "#dfz.import_data('test_data/bracken.tabular', file_type='tsv')\n",
    "#dfz.import_data('test_data/bracken.tabular', file_type='tsv')\n",
    "#dfz.import_data('test_data/TestSample2.json', file_type='json')\n",
    "dfz.show()\n",
    "# Example usage:\n",
    "#dfz.export_data('test_data/t3.json', file_type='json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status curently:\n",
    "full csv, tsv, json support\n",
    "partial xml and yaml support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# This is included at the end to ensure when you run through your notebook the code is also transferred to the associated python package\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
