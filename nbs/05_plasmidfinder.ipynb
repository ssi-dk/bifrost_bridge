{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp plasmidfinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# See above? this hides these blocks, meaning these blocks aren't in the module and aren't in the documentation\n",
    "import nbdev\n",
    "from nbdev.showdoc import *  # ignore this Pylance warning in favor of following nbdev docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# That export there, it makes sure this code goes into the module.\n",
    "\n",
    "# standard libs\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Common to template\n",
    "# add into settings.ini, requirements, package name is python-dotenv, for conda build ensure `conda config --add channels conda-forge`\n",
    "import dotenv  # for loading config from .env files, https://pypi.org/project/python-dotenv/\n",
    "import envyaml  # Allows to loads env vars into a yaml file, https://github.com/thesimj/envyaml\n",
    "import fastcore  # To add functionality related to nbdev development, https://github.com/fastai/fastcore/\n",
    "from fastcore import (\n",
    "    test,\n",
    ")\n",
    "from fastcore.script import (\n",
    "    call_parse,\n",
    ")  # for @call_parse, https://fastcore.fast.ai/script\n",
    "import json  # for nicely printing json and yaml\n",
    "from fastcore import test\n",
    "from bifrost_bridge import core\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the notebooks now are located in the `nbs` folder, we need to change the python `wd` for the notebook to the project folder. Keep this included in all notebooks but don't export it to the package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block should never be exported. It is to have python running in the project (and not the nbs) dir, and to initiate the package using pip.\n",
    "os.chdir(core.PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################CODE_SEGMENT###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def process_plasmidfinder_data(\n",
    "    input_path:str,\n",
    "    output_path:str = './output.tsv',\n",
    "    replace_header:str = None,\n",
    "    filter_columns:str = None,\n",
    "    add_header:str = None,\n",
    "    convert_coverage:bool = False,\n",
    "    filter_contig:bool = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Command-line interface for processing plasmidfinder data.\n",
    "\n",
    "    This function sets up an argument parser to handle command-line arguments for processing plasmidfinder data files.\n",
    "    It supports specifying input and output file paths, replacing headers, filtering columns, and handling the presence or absence of headers in the input file.\n",
    "\n",
    "    Arguments:\n",
    "        input_path (str): Path to the input file.\n",
    "        output_path (str): Path to the output file (default: './output.tsv').\n",
    "        replace_header (str): Header to replace the existing header (default: None).\n",
    "        filter_columns (str): Columns to filter from the header (default: None).\n",
    "        header_exists (int): Indicates if the header exists in the input file (default: 1).\n",
    "        add_header (str): Header to add if the header does not exist in the input file (default: None).\n",
    "        convert_coverage (bool): If True, converts coverage values in the 'Query / Template length' column to percentages (default: False).\n",
    "        filter_contig (bool): If True, filters out 'Contig' column to just contig number (default: False).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    df = core.DataFrame()\n",
    "    \n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"The input file {input_path} does not exist.\")\n",
    "    df.import_data(input_path, file_type='tsv', add_header=add_header)\n",
    "\n",
    "    def concatenate_vector(x, sep=','):\n",
    "        return ','.join([str(i) for i in x])\n",
    "    \n",
    "    df_agg = df.df.apply(concatenate_vector, axis=0)\n",
    "    df.df = df_agg.to_frame().T\n",
    "\n",
    "    #PFinder_Coverage contains value like \"152 / 152,682 / 682\", thats two values separated by commas, I would like to divide first number by second number and replaces the value with the result\n",
    "    def process_coverage(val):\n",
    "        # Split by comma, process each \"a / b\" part, return comma-separated results\n",
    "        parts = [v.strip() for v in str(val).split(',')]\n",
    "        results = []\n",
    "        for part in parts:\n",
    "            if '/' in part:\n",
    "                num, denom = part.split('/')\n",
    "                try:\n",
    "                    ratio = float(num.strip()) / float(denom.strip()) * 100\n",
    "                    results.append(str(ratio))\n",
    "                except Exception:\n",
    "                    results.append(part)\n",
    "            else:\n",
    "                results.append(part)\n",
    "        return ','.join(results)\n",
    "    if convert_coverage:\n",
    "        df.df['Query / Template length'] = df.df['Query / Template length'].apply(process_coverage)\n",
    "\n",
    "    def extract_contig(val):\n",
    "        # Extract the first part before any space or comma\n",
    "        parts = [v.strip() for v in str(val).split(',')]\n",
    "        results = []\n",
    "        for part in parts:\n",
    "            match = re.search(r'\\b(\\w+)\\d+\\b', part)\n",
    "            if match:\n",
    "                results.append(match.group(0))\n",
    "            else:\n",
    "                results.append(part)\n",
    "        return ','.join(results)\n",
    "    # If filter_contig is True, we look at the 'Contig' column, it contains \"textXXXX more text,text2XXXXX more text\", we want to extract textXXXX,text2XXXXX, etc\n",
    "    if filter_contig:\n",
    "        df.df['Contig'] = df.df['Contig'].apply(extract_contig)\n",
    "\n",
    "    if filter_columns:\n",
    "        df.filter_columns(filter_columns)\n",
    "\n",
    "    if replace_header:\n",
    "        df.rename_header(replace_header)\n",
    "\n",
    "\n",
    "    \n",
    "    #df.show()\n",
    "\n",
    "    df.export_data(output_path, file_type='tsv')\n",
    "\n",
    "@call_parse\n",
    "def process_plasmidfinder_data_from_cli(\n",
    "    input_path:str,\n",
    "    output_path:str = './output.tsv',\n",
    "    replace_header:str = None,\n",
    "    filter_columns:str = None,\n",
    "    add_header:str = None,\n",
    "    convert_coverage:bool = False,\n",
    "    filter_contig:bool = False):\n",
    "    process_plasmidfinder_data(input_path, output_path, replace_header, filter_columns, add_header, convert_coverage, filter_contig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#Example usage of the function\n",
    "#process_plasmidfinder_data(\n",
    "#   input_path='test_data/plasmidfinder.tsv', \n",
    "#   output_path='test_data/plasmidfinder_testout.tsv',\n",
    "#   convert_coverage=True,\n",
    "#   filter_contig=True\n",
    "   #filter_columns=\"Query / Template length\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################CODE_SEGMENT###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# This is included at the end to ensure when you run through your notebook the code is also transferred to the associated python package\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
